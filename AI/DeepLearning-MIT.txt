Deep Learning
===============
Lecture 1: introduction to deep learning
https://www.youtube.com/watch?v=ErnWZxJovaM&list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI
Neural network - simulation of human brain
Percepron - Equivalend of neuron.
	= inputs -> weights -> sum (+ bias) -> non linearity -> output
	Non linearity is achieved using non linear function.
	Non linear function - e.g. - sigmoid, hyperbolic tangent, rlt rectified linear
	Percepron can have single or multiple output.
	--------------
	Weight vs bias
	-------------
	Mathematical Representation
	For a single neuron:
		Output = f(w1X1 + w2x2 + w3x3 ...wnxn + b)
	where x is input, w is weight, b is bias, f is activation function.
	Weights control the impact of inputs (scaling factor).
	Bias allows shifting the function (adjustment factor).
	Both work together to help the neural network learn complex patterns effectively
	-----------------
	
Trained neural network -> neural network with weights
Loss functions -
	Loss - 	cost incurred from incorrect prediction. e.g what are my chances of passing the test
	Empirical loss - total loss over entire dataset. e.g what are the chances of all our batchmates passing the test
	Cross entropy loss - used for models that predicts values between 0..1
	Binary cross entropy loss - used for models that predicts values like Yes/No
	Mean squared error loss - used with regression model that outputs continuous real numbers
	
	
	Based on the type of model, we may use different tf function

Loss optimization - finding weights to minimize loss.
	Loss functions are functions of network weights.

Gradient - actual/predicted
Gradient Descent algorithm
	Initialize weights randomly, 
	calculate gradient, 
	keep moving till gradientin minimum
	get weights
Stochastic Gradient Descent algorithm -
	 Stochastic Gradient Descent converges quickly but has high noise(stochastic), 
		(because it was computed using just one example
	 whereas Gradient Descent converges slowly but has low noise
	 So choose number of examples optimally. Common practice is 32.
	 We can do parallelizm on this 32 set and put on different GPUs to make it run faster.
Back propagation
	How does a small change in one weight affects the final loss
GD and BP both help in improving the prediction accuracy of neural network
	but how are they different? - We compute the gradient using back propagation.
	
Learning Rate -  is a hyperparameter in neural networks that controls 
	how quickly an algorithm learns or updates the values of a parameter estimate.
	Keep the LR very small - LR converges slowly and get stuck in local minima
	Keep the LR very large - overshoots and diverge
	Keep the LR optimum
	LR is multiplied to the formula of actual/predicted
	How to find out LR
		-Random - keep applying and see what gives better result
		-Design an adaptive learning rate.
	Algorithms
		SGD
		Adam
		Adadelta
		Adagrad
		RMSProp

Overfitting in neural network 
	- simptom - model performed well on training dataset, but very badly on test data
	- dont underfit or overfit. Do ideal fit.

Regularization - Technique that constrians optimization problem to discourage complex models.
			Drop outs - set some activation to '0'. (kill some neurons)
			Early stopping - Calculate loss of both test and train data. Stop at a point
				where overfitting starts on test data.

Codes: Lecture 1 - 31.20, 49, 58.19 
================================================




Lecture 2: Recurrent neural network, Transformers and attention
https://www.youtube.com/watch?v=dqoEU9Ac3ek&list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI&index=2

Why sequence is important. 
	Sequence is all around us. When we speak, read something,
	DNA, protien sequence, stock market price, ECG etc.
	We need to build a model which can predict next thing based on previous things happened.

Neurons with recurrence
	Apply a recurrence relation at every time step to process a sequence.
	ht 			= fw(xt, ht-1)
	cell state 	= function with weights w(input, old state)
		note that old state(context) is also considered while calculating cell state.
RNN - input vector -> RNN(recurrent cell) -> output vector
	Total Loss - sum of losses at each step
	RNNs for sequence modeling (x is input, y is output)
		x to y			 -	Binary classification
		x to many y		 - 	Sentiment classification
		many x to y		 - 	Text generation, image capturing
		many x to many y -  translation and forcasting, music generation

			[Sentiment classification- 
			 automated process of identifying and classifying emotions in text as 
			 positive sentiment, negative sentiment, or neutral sentiment based on 
			 the opinions expressed within]
Sequence modeling: Design criteria
	To model sequence we need to
		- handle variable length sequence
		- track long term dependencies (memory)
		- maintain information about order (order matters in sequence)
		- share params across sequence
	RNN meets all criteria. It is very good at some, okay at other.

Encoding language for neural network
	Neural network can't interpret words. They require numerical inputs.
	This is done through embedding.
	Embedding - Transforming index into a fixed size vector (remember NLP)
		One hot embedding - fix size
		Learned Embedding - words plottel in x, y axis. Similar words closer to each other.
		
Backpropagation through time(BPTT)
		
	
		
Codes: 14.16, 20.40
	
------------
antromorphism
linear algebra, chain rule of derivatives, probability distribution
--------------	

stable diffution
allen turing
imitation game
